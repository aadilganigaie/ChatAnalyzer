{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadilganigaie/ChatAnalyzer/blob/main/Filtersize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#import keras_tuner\n",
        "#from keras_tuner.tuners import RandomSearch\n",
        "import nlpaug as naw\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.stem.porter import PortorStemmer\n",
        "import re\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "import contractions\n",
        "#import fasttext\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, GlobalMaxPooling1D, Flatten, Conv1D, MaxPool1D, Reshape\n",
        "\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "toke = Tokenizer()\n",
        "import contractions\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "_f3xGsFH_Ht9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train-balanced-sarcasm.csv', error_bad_lines= False)\n",
        "df"
      ],
      "metadata": {
        "id": "MvVix-OMTdP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()\n"
      ],
      "metadata": {
        "id": "9F-aFE9zTdNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(df['label'])\n",
        "labels"
      ],
      "metadata": {
        "id": "ptqjCTItTdKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = ' '.join([word for word in text.split(' ') if word not in stop_words])\n",
        "    text =  text.encode('ascii', 'ignore').decode()\n",
        "    text = re.sub(\"@\\S+\", \" \", text)\n",
        "    text = re.sub(\"https*\\S+\", \" \", text)\n",
        "    text = re.sub(\"#\\S+\", \" \", text)\n",
        "    text = re.sub(\"\\'\\w+\", '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
        "    text = re.sub('\\s{2,}', \" \", text)\n",
        "    return text\n",
        "  df['clean_text'] = df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "Ogt_ZTMbTdHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['no_contrac'] = df['clean_text']. apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
        "df['no_contrac_str'] = [' '.join(map(str, l)) for l in df['no_contrac']]"
      ],
      "metadata": {
        "id": "CZUaY-qRTdE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lem_sent'] = [lemmatizer.lemmatize(words_sent) for words_sent in df['clean_text']]"
      ],
      "metadata": {
        "id": "MqPSgaFKTdB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['lem_sent'],df['label'], test_size = 0.2, shuffle= False)"
      ],
      "metadata": {
        "id": "hLKmlPe-Tc_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toke.fit_on_texts(x_train)\n",
        "vocab_size = len(toke.word_index) +1\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "Wk_cY3ZNTc5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = toke.texts_to_sequences(x_train)\n",
        "x_test = toke.texts_to_sequences(x_test)"
      ],
      "metadata": {
        "id": "6zJNra6TUQiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pad = pad_sequences(x_train, padding = 'post', maxlen = 200)\n",
        "x_test_pad = pad_sequences(x_test, padding = 'post', maxlen = 200)"
      ],
      "metadata": {
        "id": "gDi7JbCVUQfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.models import Sequential\n",
        "\n",
        "# Set the filter sizes you want to try\n",
        "filter_sizes = [2, 3, 4]\n",
        "\n",
        "# Set the number of filters you want to try\n",
        "num_filters_list = [32, 64, 128]\n",
        "\n",
        "# Create an empty list to store the models\n",
        "models = []\n",
        "history = []\n",
        "\n",
        "# Loop over each filter size\n",
        "for filter_size in filter_sizes:\n",
        "    # Loop over each number of filters\n",
        "    for num_filters in num_filters_list:\n",
        "        # Define the model\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=200))\n",
        "        model.add(Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu'))\n",
        "        model.add(GlobalMaxPooling1D())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # Compile the model\n",
        "        history = model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        # Add the model to the list\n",
        "        models.append(model)\n",
        "\n",
        "# Fit the models\n",
        "for model in models:\n",
        "    model.fit(x_train_pad, y_train, validation_data=(x_test_pad, y_test), epochs=10, batch_size=64)\n",
        "\n",
        "# Evaluate the models\n",
        "for model in models:\n",
        "    print(model.evaluate(x_test_pad, y_test))\n"
      ],
      "metadata": {
        "id": "ZO78VXy6UQcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loop over the models\n",
        "for i, model in enumerate(models):\n",
        "    # Get the training and validation loss and accuracy for each epoch\n",
        "    loss = model.history.history['loss']\n",
        "    val_loss = model.history.history['val_loss']\n",
        "    acc = model.history.history['accuracy']\n",
        "    val_acc = model.history.history['val_accuracy']\n",
        "\n",
        "    # Plot the training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(loss, label='Training loss')\n",
        "    plt.plot(val_loss, label='Validation loss')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Model {} Loss'.format(i+1))\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the training and validation accuracy\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(acc, label='Training accuracy')\n",
        "    plt.plot(val_acc, label='Validation accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Model {} Accuracy'.format(i+1))\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "KewVLFtSUQZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "y_pred = models[0].predict(x_test_pad)\n",
        "\n",
        "# Convert the continuous-multioutput predictions to one-hot encoded predictions\n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "\n",
        "# Get the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot = True)"
      ],
      "metadata": {
        "id": "pGmdEYcqUQXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "\n",
        "# Loop over the models\n",
        "for i, model in enumerate(models):\n",
        "    # Get the predictions for the test set\n",
        "    y_pred = model.predict(x_test_pad)\n",
        "\n",
        "    # Convert the continuous-multioutput predictions to one-hot encoded predictions\n",
        "    y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "    # Get the confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 3))\n",
        "\n",
        "    sns.heatmap(cm/np.sum(cm), annot=True,\n",
        "            fmt='.2%')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SXCiSGF_Ui0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVoPMs6HRfsrQUumZKjQYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}